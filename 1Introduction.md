PART-I
Intelligence 
Psychologists generally do not characterize human intelligence by just one trait but by the combination of many diverse abilities; higher level abilities (such as abstract reasoning, mental representation, problem solving, and decision making), the ability to learn, emotional knowledge, creativity, and adaptation to meet the demands of the environment effectively. Research in AI has focused chiefly on the following components of intelligence: learning, reasoning, problem solving, perception, and using language.

Artificial Intelligence (AI)
Artificial intelligence (AI) is a wide-ranging branch of computer science concerned with building smart machines capable of performing tasks that typically require human intelligence. It deals with helping machines finding solutions to complex problems in a more human-like fashion. This generally involves borrowing characteristics from human intelligence, and applying them as algorithms in a computer friendly way.

AI systems work by ingesting large amounts of labeled training data, analyzing the data for correlations and patterns, and using these patterns to make predictions about future states. In this way, a chatbot that is fed examples of text chats can learn to produce lifelike exchanges with people, or an image recognition tool can learn to identify and describe objects in images by reviewing millions of examples.

AI programming focuses on three cognitive skills: learning, reasoning and self-correction.
    •	Learning processes. This aspect of AI programming focuses on acquiring data and creating rules for how to turn the data into actionable information. The rules, which are called algorithms, provide computing devices with step-by-step instructions for how to complete a specific task.
    •	Reasoning processes. This aspect of AI programming focuses on choosing the right algorithm to reach a desired outcome.
    •	Self-correction processes. This aspect of AI programming is designed to continually fine-tune algorithms and ensure they provide the most accurate results possible.
Advantages
    •	Good at detail-oriented jobs;
    •	Reduced time for data-heavy tasks;
    •	Delivers consistent results; and
    •	AI-powered virtual agents are always available.
Disadvantages
    •	Expensive;
    •	Requires deep technical expertise;
    •	Limited supply of qualified workers to build AI tools;
    •	Only knows what it's been shown; and
    •	Lack of ability to generalize from one task to another.

AI Perspectives: acting and thinking humanity, acting and thinking rationally
Some consider intelligence to be a property of internal thought processes and reasoning, while others focus on intelligent behavior, an external characterization.
From these two dimensions—human vs. rational and thought vs. behavior—there are four possible combinations
1.	Acting humanly: The Turing test approach
2.	Thinking humanly: The cognitive modeling approach
3.	Thinking rationally: The “laws of thought” approach
4.	Acting rationally: The rational agent approach

1.	Acting humanly: The Turing test approach
The first proposal for success in building a program and acts humanly was the Turing Test, proposed by Alan Turing (1950). To be considered intelligent, a program must be able to act sufficiently like a human to fool an interrogator. A human interrogates the program and another human via a terminal simultaneously. If after a reasonable period, the interrogator cannot tell which is which, the program passes.
To pass this test requires:
    ●	natural language processing: to enable it ro communicate successfully in English.
    ●	knowledge representation: to store what it knows or hears.
    ●	automated reasoning: to use the stored information to answer questions and to draw new conclusions.
    ●	machine learning: to adapt to new circumstances and to detect and extrapolate patterns.
This test avoids physical contact and concentrates on "higher level" mental faculties, physical simulation of a person is unnecessary for intelligence. However, a total Turing test includes a video signal so that the interrogator can test the subjects perceptual abilities as well as the opportunity for the intelligence to pass physical objects ”through the hatch”. To pass the total Turing test, it would require the program to also do:
    ●	computer vision: to perceive objects.
    ●	Robotics: to manipulate object and move about.
    ●	Thinking Humanly
This requires "getting inside" of the human mind to see how it works and then comparing our computer programs to this. This is what cognitive science attempts to do. Another way to do this is to observe a human problem solving and argue that one's programs go about problem solving in a similar way.
Example: GPS (General Problem Solver) was an early computer program that attempted to model human thinking. The developers were not so much interested in whether or not GPS solved problems correctly. They were more interested in showing that it solved problems like people, going through the same steps and taking around the same amount of time to perform those steps.

2.	Thinking Humanly: The cognitive modeling approach
This requires "getting inside" of the human mind to see how it work. There are two ways to do this:
    •	Through introspection – trying to catch our own thoughts as they go by and observe a human problem solving and argue that one’s proamgs go about problem solving in a similar way.
    •	Through psychological experiments – this is what cognitive science attempts to do. 
Once we have a sufficiently precise theory of mind, it becomes possible to express the theory as a computer program. 
Example: GPS (General Problem Solver) was an early computer program that attempted to model human thinking. The developers were not so much interested in whether or not GPS solved problems correctly. They were more interested in showing that it solved problems like people, going through the same steps and taking around the same amount of time to perform those steps.

    **Note: 
        The interdisciplinary field of cognitive science brigs together computer models from AI and experimental techniques from psychology to try to construct precise and testable theories of the workings of the human mind. 

3.	Thinking Rationally: The “laws of though” approach
Aristotle was one of the first to attempt to codify "thinking". His syllogisms provided patterns of argument structure that always gave correct conclusions, giving correct premises.
Example: All computers use energy. Using energy always generates heat. Therefore, all computers generate heat.
This study initiated the field of logic. Formal logic was developed in the late nineteenth century. This was the first step toward enabling computer programs to reason logically.
There are two main obstacles to this approach: 
    ●	First, it is difficult to make informal knowledge precise enough to use the logicist approach particularly when there is uncertainty in the knowledge.
    ●	Second, there is a big difference between being able to solve a problem in principle and doing so in practice.
Although both of these obstacles apply to any attempt to build computational reasoning systems, they appeared first in the logicist tradition. 

4.	Acting Rationally: The rational agent approach
Acting rationally means acting so as to achieve one's goals, given one's beliefs. An agent is just something that perceives and acts.
In the logical approach to AI, the emphasis is on correct inferences. This is often part of being a rational agent because one way to act rationally is to reason logically and then act on one’s conclusions. But this is not all of rationality because agents often find themselves in situations where there is no provably correct thing to do, yet they must do something.
There are also ways to act rationally that do not seem to involve inference, e.g., reflex actions.
The study of AI as rational agent design has two advantages:
    ●	It is more general than the logical approach because correct inference is only a useful mechanism for achieving rationality, not a necessary one.
    ●	It is more amenable to scientific development than approaches based on human behaviour or human thought because a standard of rationality can be clearly defined and completely general, being independent of humans.
Achieving perfect rationality – always doing the right thing – is not feasible in complicated environments, because the computational demands re just too high. 

PART-II
History of AI
The late 19th and first half of the 20th centuries brought forth the foundational work that would give rise to the modern computer. In 1836, Cambridge University mathematician Charles Babbage and Augusta Ada Byron, Countess of Lovelace, invented the first design for a programmable machine.

•	1940s. Princeton mathematician John Von Neumann conceived the architecture for the stored-program computer -- the idea that a computer's program and the data it processes can be kept in the computer's memory. And Warren McCulloch and Walter Pitts laid the foundation for neural networks.

•	1950s. With the advent of modern computers, scientists could test their ideas about machine intelligence. One method for determining whether a computer has intelligence was devised by the British mathematician and World War II code-breaker Alan Turing. 

•	1956. The modern field of artificial intelligence is widely cited as starting this year during a summer conference at Dartmouth College. Sponsored by the Defense Advanced Research Projects Agency (DARPA), the conference was attended by 10 luminaries in the field, including AI pioneers Marvin Minsky, Oliver Selfridge and John McCarthy, who is credited with coining the term artificial intelligence. Also in attendance were Allen Newell, a computer scientist, and Herbert A. Simon, an economist, political scientist and cognitive psychologist, who presented their groundbreaking Logic Theorist, a computer program capable of proving certain mathematical theorems and referred to as the first AI program.

•	1950s and 1960s. In the late 1950s, Newell and Simon published the General Problem Solver (GPS) algorithm, which fell short of solving complex problems but laid the foundations for developing more sophisticated cognitive architectures; McCarthy developed Lisp, a language for AI programming that is still used today. In the mid-1960s MIT Professor Joseph Weizenbaum developed ELIZA, an early natural language processing program that laid the foundation for today's chatbots.

•	1970s and 1980s. Government and corporations backed away from their support of AI research, leading to a fallow period lasting from 1974 to 1980 and known as the first "AI Winter." In the 1980s, research on deep learning techniques and industry's adoption of Edward Feigenbaum's expert systems sparked a new wave of AI enthusiasm, only to be followed by another collapse of government funding and industry support. The second AI winter lasted until the mid-1990s.

•	1990s through today. Increases in computational power and an explosion of data sparked an AI renaissance in the late 1990s that has continued to present times. The latest focus on AI has given rise to breakthroughs in natural language processing, computer vision, robotics, machine learning, deep learning and more. Moreover, AI is becoming ever more tangible, powering cars, diagnosing disease and cementing its role in popular culture. More recently, the historic defeat of 18-time World Go champion Lee Sedol by Google DeepMind's AlphaGo stunned the Go community and marked a major milestone in the development of intelligent machines.

PART-III
Foundations of AI
The disciplines that contributed ideas, viewpoints, and techniques to AI. It is forced to concentrate on a small number of people, events, and ideas and to ignore others that also were important. I’ll explain and represent it through a series of questions

1.	Philosophy
    •	Can formal rules be used to draw valid conclusions?
    •	How does the mind arise from a physical brain?
    •	Where does knowledge come from?
    •	How does knowledge lead to action?
Rationalism, Dualism, Materialism, Empiricism, Induction, Logical Positivism, Confirmation Theory.
2.	Economics
    ●	How should we make decisions in accordance with our preferences? 
    ●	How should we do this when others may not go along?
    ●	How should we do this when the payoff may be far in the future?
Utility, Decision Theory, Game Theory, Operations Research.
3.	Psychology
    ●	How do brains process information?
Behaviourism, Cognitive psychology.
The three key steps of a knowledge-based agent:
    •	the stimulus must be translated into an internal representation
    •	the representation is manipulated by cognitive processes to derive internal representations
    •	These are in turn retranslated back into action.
4.	Sociology
5.	Linguistics
    ●	How does language relate to thought?
    ●	Verbal Behavior — behaviorist approach to language learning
Computational linguistics or natural language processing and knowledge representation.
6.	Neuroscience
    ●	How do brains process information?
Neuroscience is the study of the nervous system, especially the brain. We are still a long way from understanding how cognitive processes actually work. 
7.	Mathematics
    ●	What are the formal rules to draw valid conclusions? 
    ●	What can be computed? 
    ●	How do we reason with uncertain information?
The main three fundamental areas are logic, computation and probability.
Algorithm, incompleteness theorem, computable, tractability, NP completeness, Non deterministic polynomial and probability.
8.	Computer Science
    ●	How do brains process information?
Operational computer and operational programmable computer
AI has pioneered many ideas that have made their way back to mainstream computer science, including time sharing, interactive interpreters, personal computers with windows and mice, rapid development environments, the linked list data type, automatic storage management, and key concepts of symbolic, functional, declarative, and object-oriented programming.
9.	Control Theory
    ●	How can artifacts operate under their own control?
control Theory, Homeostatic and objective function

PART-V
Applications of AI
Artificial intelligence has made its way into a wide variety of markets. Here are nine examples.
AI in healthcare. 
    •	To improve patient outcomes (better and faster diagnoses than humans) and reducing costs. 
    •	One of the best-known healthcare technologies is IBM Watson. It understands natural language and can respond to questions asked of it. The system mines patient data and other available data sources to form a hypothesis, which it then presents with a confidence scoring schema. 
    •	Other AI applications include using online virtual health assistants and chatbots to help patients and healthcare customers find medical information, schedule appointments, understand the billing process and complete other administrative processes. 
    •	An array of AI technologies is also being used to predict, fight and understand pandemics such as COVID-19.
AI in business.
    •	Machine learning algorithms are being integrated into analytics and customer relationship management (CRM) platforms to uncover information on how to better serve customers. 
    •	Chatbots have been incorporated into websites to provide immediate service to customers. 
    •	Automation of job positions.
AI in education. 
    •	AI can automate grading, giving educators more time. 
    •	AI can assess students and adapt to their needs, helping them work at their own pace. 
    •	AI tutors can provide additional support to students, ensuring they stay on track. And it could change where and how students learn, perhaps even replacing some teachers.
AI in finance. 
    •	AI in personal finance applications, such as Intuit Mint or TurboTax, collect personal data and provide financial advice. 
    •	Other programs, such as IBM Watson, have been applied to the process of buying a home. 
    •	Today, artificial intelligence software performs much of the trading on Wall Street.
AI in law. 
    •	The discovery process -- sifting through documents -- in law is often overwhelming for humans. Using AI to help automate the legal industry's labor-intensive processes is saving time and improving client service. 
    •	Law firms are using machine learning to describe data and predict outcomes, computer vision to classify and extract information from documents and natural language processing to interpret requests for information.
AI in manufacturing. 
    •	Incorporating robots into the workflow of manufacturing. For example, the industrial robots that were at one time programmed to perform single tasks and separated from human workers, increasingly function as cobots: Smaller, multitasking robots that collaborate with humans and take on responsibility for more parts of the job in warehouses, factory floors and other workspaces.
AI in banking. 
    •	Employing chatbots to make their customers aware of services and offerings and to handle transactions that don't require human intervention. 
    •	AI virtual assistants are being used to improve and cut the costs of compliance with banking regulations. 
    •	To improve decision-making for loans, and to set credit limits and identify investment opportunities.
AI in transportation. 
    •	To manage traffic, predict flight delays, and make ocean shipping safer and more efficient.
Security.
    •	Organizations use machine learning in security information and event management (SIEM) software and related areas to detect anomalies and identify suspicious activities that indicate threats. 
    •	By analyzing data and using logic to identify similarities to known malicious code, AI can provide alerts to new and emerging attacks much sooner than human employees and previous technology iterations. 
    •	To fight off cyber attacks.
